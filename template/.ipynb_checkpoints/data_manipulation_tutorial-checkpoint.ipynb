{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Conhecendo o repositório + Case de manipulação__\n",
    "\n",
    "Implementaremos algumas das funções que estão nos scripts do repositório com objetivo de absorver melhor sua lógica.\n",
    "À seguir, chamaremos as principais classes do repositório configurando as credenciais e preparando as interfaces com bases de dados, e em seguida extrairemos a lista de datas já carregadas de um determinado contexto.\n",
    "\n",
    "IMPORTANTE: É necessário seguir o contexto indicado para que o controle de valores de um ETL não afete o outro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Como fazer consultas à API do data lake sem usar as classes do repositório__ (implementaremos apenas o que é mais trivial)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "username = 'atelles'\n",
    "password = '' # pegar nos slides de setup\n",
    "endpoint = 'http://prestoapides.olxdev.io:5000/api/query'\n",
    "query = 'select * from ods.dm_category'\n",
    "content_type =\"text/plain\"\n",
    "header = {'Content-Type':content_type}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = requests.Session()\n",
    "r = session.post(self.endpoint,auth = (self.username,self.password), headers = self.header, data = self.query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = r.json()\n",
    "df = pd.DataFrame(json_data)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Replicaremos agora o de forma simples a função de controle de valores__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, traceback\n",
    "#change to the utils folder\n",
    "sys.path.append(os.path.abspath(os.path.join('..', 'utils')))\n",
    "\n",
    "from query_executor import query\n",
    "import config\n",
    "import pandas as pd\n",
    "import re\n",
    "import math\n",
    "import time\n",
    "import emailutil\n",
    "#import math\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "from get_values_to_load import get_days_to_load\n",
    "import string_manipulators\n",
    "\n",
    "# get configuration info from config.ini through config.py\n",
    "pg_config_dict = config.get_all_as_dict('postgres')\n",
    "presto_config_dict = config.get_all_as_dict('presto')\n",
    "\n",
    "# instantiate classes and configure connections\n",
    "postgres_executor = query(pg_config_dict)\n",
    "presto_executor = query(presto_config_dict)\n",
    "\n",
    "date_log_table = 'etl_class.date_log'\n",
    "context = 'assignment1'\n",
    "\n",
    "log_query = f\"\"\"select date from {date_log_table} where context = '{context}' order by date desc\"\"\"\n",
    "\n",
    "pg_result = postgres_executor.query_jdbc(log_query)\n",
    "\n",
    "already_loaded_dates = [pd.to_datetime(date).date() for date in pg_result['date'].values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podem observar, todos estão com as datas 26/08 e 28/08 salvas.\n",
    "Agora geraremos o range de datas a serem carregadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = pd.to_datetime('2018-08-01')\n",
    "end_date = pd.to_datetime('2018-09-15')\n",
    "desired_load_list = pd.date_range(start=start_date, end=end_date).date.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nessa última etapa, checamos quais datas foram carregadas daquelas que estão na lista das desejadas pelo ETL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_to_load = [date for date in desired_load_list if date not in already_loaded_dates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_to_load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Abordaremos agora a criação de partições a partir de um range de datas, tarefa comum no nosso ambiente de dados com tabelas particionadas__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_d = pd.to_datetime('2018-12-30')\n",
    "end_d = pd.to_datetime('2019-02-10')\n",
    "desired_date_list = pd.date_range(start=start_d, end=end_d).date.tolist()\n",
    "        \n",
    "dates = pd.DataFrame(desired_date_list,columns=['dates']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usaremos o índice do dataframe como referência para agrupar os dias que estão na coluna day por mês e ano, concatenando todos os dias com vírgula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates['day'] = dates['dates'].apply(lambda x: str(x.day))\n",
    "dates['month'] = dates['dates'].apply(lambda x: str(x.month))\n",
    "dates['year'] = dates['dates'].apply(lambda x: str(x.year))\n",
    "grouped = dates.groupby(['year','month'])['day'].apply(lambda x: \"%s\" % ', '.join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iteraremos agora sobre o dataframe construindo a string de acordo com o agrupamento feito anteriormente, usando o índice para construir ano e mes e os valores com os dias agregados para a lista de dias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_count = 0\n",
    "for year,month in list(grouped.index):\n",
    "    if month_count == 0:\n",
    "        date_string = \"(year = \" + str(year) + \" and month = \" + str(month) + \" and day in (\" \\\n",
    "        + str(grouped[year,month]) + \"))\"\n",
    "    else:\n",
    "        date_string += \" or (year = \" + str(year) + \" and month = \" + str(month) + \" and day in (\" \\\n",
    "        + str(grouped[year,month]) + \"))\"\n",
    "    month_count += 1\n",
    "date_string = \"(\" + date_string + \")\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Case de manipulação de dados com pandas__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importando as bibliotecas necessárias e armazenando as credenciais e queries em memória."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu Dec  6 16:53:57 2018\n",
    "\n",
    "@author: arthur.telles\n",
    "\"\"\"\n",
    "\n",
    "import sys, os, traceback\n",
    "#change to the utils folder\n",
    "sys.path.append(os.path.abspath(os.path.join('..', 'utils')))\n",
    "\n",
    "from query_executor import query\n",
    "import config\n",
    "import pandas as pd\n",
    "import re\n",
    "import math\n",
    "import time\n",
    "import emailutil\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "from get_values_to_load import get_days_to_load\n",
    "import string_manipulators\n",
    "\n",
    "initial_time = time.time()\n",
    "\n",
    "# get configuration info from config.ini through config.py\n",
    "pg_config_dict = config.get_all_as_dict('postgres')\n",
    "presto_config_dict = config.get_all_as_dict('presto')\n",
    "\n",
    "# instantiate classes and configure connections\n",
    "postgres_executor = query(pg_config_dict)\n",
    "presto_executor = query(presto_config_dict)\n",
    "\n",
    "query_to_execute = \"\"\"select approval_date,\n",
    "b.list_id_nk,\n",
    "reason_removed_detail_name,\n",
    "state_name,\n",
    "case when b.category_id_fk = 46 then 'Autos'\n",
    "     when b.category_id_fk in (79,40,86,44,3,2,80) then 'RE'\n",
    "     when b.category_id_fk is null then 'no cat'\n",
    "     else 'Other' end as category,\n",
    "account_id_fk,\n",
    "price\n",
    "from ods.ad b\n",
    "join ods.dm_area c on b.area_id_fk=c.area_id_pk\n",
    "join ods.dm_reason_removed_detail d on b.reason_removed_detail_id_fk=d.reason_removed_detail_id_pk\n",
    "where {DATE_CLAUSE}\"\"\"\n",
    "\n",
    "# ---- Define these variables ---- #\n",
    "etl_context = 'assignment12'\n",
    "schema_to_load = 'etl_class'\n",
    "table_in_schema_to_load = 'assignment'\n",
    "# -------------------------------- #\n",
    "\n",
    "\n",
    "print(\"\\n Running ETL # -------------- # \\n \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chamaremos a função que vai carregar os dias indicados pelo contexto escolhido anteriormente, em seguida será feita a exploração de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = pd.to_datetime('2018-08-25')\n",
    "dates_to_load, last_day_loaded = get_days_to_load(etl_context,'etl_class.date_log',\\\n",
    "                                                    'etl_class.config_params',\\\n",
    "                                                    last_day=pd.to_datetime('2018-09-01'))\n",
    "\n",
    "# Loading at D-1\n",
    "date_clause_string, today_date_string = string_manipulators.generate_date_string(date - timedelta(days=1),False)\n",
    "\n",
    "edited_query = string_manipulators.substitute_params_in_string(['{DATE_CLAUSE}'],\\\n",
    "                                            [date_clause_string],query_to_execute)\n",
    "\n",
    "presto_result = presto_executor.query_request(edited_query)\n",
    "\n",
    "pg_result = postgres_executor.query_jdbc(\"select * from etl_class.accounts_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg_result.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "presto_result.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg_result.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Próximo passo é executar o join e avaliar quais ids não estão presentes no segundo dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.merge(presto_result,pg_result,how='left',on='account_id_fk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É necessário restringir os dados para ads aprovados em 2018-08-25 pois o year month day não limita todos os approval dates a essa data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result2 = result[(~pd.isnull(result['gender'])) & (result['approval_date'] == '2018-08-25')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemplo de uso da função apply: Conversão de todos os valores 't' e 'f' para True e False, com objetivo de otimizar o armazenamento desses dados. Operações feitas com dados booleanos se tornam exponencialmente mais rápidas que quando armazenados como string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result2['is_company'] = result2['is_company'].apply(lambda x: True if x == 't' else False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agruparemos então os dados com a função groupby, semelhante ao SQL, para trazer um pouco do processamento para memória e então extrair a métrica que desejávamos no começo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result3 = result2.groupby(['approval_date','state_name','gender','is_company'])['account_id_fk'].\\\n",
    "nunique().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Renomearemos e checaremos o formato das colunas para bater com o nome e o formato no banco. Em seguida, agruparemos também pelo list_id para retirar a quantidade de listings únicos em cada grupo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result3.columns = ['approval_date', 'state_name', 'gender', 'is_company', 'accounts']\n",
    "result3['listings'] = result2.groupby(['approval_date','state_name','gender','is_company'])['list_id_nk'].\\\n",
    "nunique().reset_index()['list_id_nk']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos fazer um teste de inserção para ver se os dados estão ok e se o banco está apto a recebê-los sem erro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = result3\n",
    "\n",
    "if result.shape[0] > 100:\n",
    "    postgres_executor.insert_chunks_with_progress(result, table_in_schema_to_load, schema_to_load,\\\n",
    "                                                if_exists='append')\n",
    "else:\n",
    "    postgres_executor.insert_jdbc(result,table_in_schema_to_load,schema_to_load, if_exists='append')\n",
    "\n",
    "date_log_data = pd.DataFrame(data={\"date\": date,\n",
    "                    \"load_date\": str(pd.to_datetime('now').tz_localize('UTC').tz_convert('America/Sao_Paulo')\\\n",
    "                                .tz_localize(None)),\n",
    "                    \"context\": etl_context}, index=[0])\n",
    "postgres_executor.insert_jdbc(date_log_data,'date_log','etl_class', if_exists='append')\n",
    "\n",
    "print('No more days to load!')\n",
    "\n",
    "# gravar no log o tempo para execução\n",
    "print(\"Total Elapsed Time: {} minutes and {} seconds.\".format(str(math.floor((time.time() - initial_time)/60)),\\\n",
    "  str(time.time() - initial_time - math.floor((time.time() - initial_time)/60)*60))) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
